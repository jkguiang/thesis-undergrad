\begin{tabular}{p{.25\linewidth}p{.75\linewidth}}
\toprule
Variable & Definition \\
\midrule
\verb|objective| & Learning objective. (`binary:logistic': logistic regression for binary classification, output probability) \\
\midrule
\verb|eta| & Step size shrinkage used in update to prevents overfitting. (range: $[0,1]$) \\
\midrule
\verb|max_depth| & Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. (range: $[0,\infty]$) \\
\midrule
\verb|verbosity| & Verbosity of printing messages. Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug). \\
\midrule
\verb|nthread| & Number of parallel threads used to run XGBoost. \\
\midrule
\verb|eval_metric| & Evaluation metrics for validation data. (`auc': Area under the curve) \\
\midrule
\verb|subsample| & Subsample ratio of the training instances. (range: $(0,1]$) \\
\midrule
\verb|alpha| & L1 regularization term on weights. Increasing this value will make model more conservative. \\
\midrule
\verb|gamma| & Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger \verb|gamma| is, the more conservative the algorithm will be. (range: $[0,\infty]$) \\
\midrule
\verb|lambda| & L2 regularization term on weights. Increasing this value will make model more conservative. Normalized to number of training examples. \\
\midrule
\verb|min_child_weight| & Minimum sum of instance weight (hessian) needed in a child. The larger \verb|min_child_weight| is, the more conservative the algorithm will be. \\
\midrule
\verb|colsample_bytree| & Subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed. \\
\bottomrule
\end{tabular}