%
%  Search for Rare Higgs Decays
%

I begin with a brief discussion of the motivation behind looking for rare Higgs decays like $H \rightarrow \rho\gamma$ and $H \rightarrow \phi\gamma$, specifically via the associated $WH$ rather than direct production. Then, I outline the event selection methods and relevant backgrounds. Finally, I describe the Boosted Decision tree we used in detail, followed by the results of the analysis.

\begin{section}{Motivation}\label{motivation}

The Standard Model branching ratios for the $H\rightarrow\phi\gamma$ and $H\rightarrow\rho\gamma$ decays are expected to be $2.31\times10^{-6}$ and $1.68\times10^{-5}$  respectively\cite{cite-sm-brs}. Put simply, they are \textit{very} rare - so rare, in fact, that they have never been directly observed. Furthermore, by any measure, these decays should never be detectable at current, human-reachable energies, that is, unless there are yet-undiscovered processes that enhance the rate of either of these decays. This implies that this measurement is experimentally exciting, because any significant measurement of the production of $\rho$ or $\phi$ mesons would be direct evidence of the existence of new physics. At the same time, if no $\rho$ or $\phi$ mesons are found, then the branching ratio of either decay mode can be pushed farther back, depending on the detector's sensitivity to the signature.

Now, ideally, we would search for these decays via \textit{direct} production, that is, when a Higgs boson is produced by gluon-gluon fusion (Fig. \ref{fig:direct-prod}), then decays to a photon and a $\phi$ or $\rho$ meson. However, CMS does not have a dedicated trigger for detecting pions or kaons (the final-state products of the $\phi$ and $\rho$ mesons respectively), so the only possibly detectable particle would be the photon. However, since the Higgs mass is 125 GeV, the photon and meson both have a momentum of approximately 60 GeV at most, so the photon is well below the threshold of the CMS single photon trigger\cite{cite-hlt}. Therefore, we chose an associated production mode, namely, $W^{\pm}+H$ (Fig. \ref{fig:whiggs-prod}), where $W \rightarrow \ell^{\pm}+\nu_{\ell}$. We can easily trigger on the leptons produced by the $W$ boson, and the Higgs comes for free. At the same time, we lose a \textit{lot} of signal, since this process is far more rare. We also have to deal with more background, since we are including more particles in the signature we are searching for. Ultimately these consequences consolidate into an overall loss of sensitivity, but alternative associated productions ($Z+H$, for instance) face similar challenges.

\begin{figure}[htb]
\begin{center}
\subfloat      {
\scalebox{.45}[0.45]{\input{Dissertation/fig/higgs-direct.tex}}
}\quad
\subfloat      {
\scalebox{.45}[0.45]{\input{Dissertation/fig/higgs-qloop.tex}}
}
\end{center}
\caption{Feynman diagram\cite{cite-tikz-feynman} for primary mechanism of the direct production of the Higgs boson at the LHC where $H \rightarrow \rho\gamma$ or $H \rightarrow \phi\gamma$.}
\label{fig:direct-prod}
\end{figure}

\begin{figure}[htb]
\begin{center}
\subfloat      {
\scalebox{.55}[0.55]{\input{Dissertation/fig/whiggs-direct.tex}}
}\quad
\subfloat      {
\scalebox{.55}[0.55]{\input{Dissertation/fig/whiggs-qloop.tex}}
}
\end{center}
\caption{Feynman diagram\cite{cite-tikz-feynman} for primary mechanism of the associated $WH$ production at the LHC studied in this analysis.}
\label{fig:whiggs-prod}
\end{figure}

\end{section}

\begin{section}{Event Selection}
\begin{subsection}{Data Aquisition}\label{data-aquisition}
The analysis begins with data based on a sample of proton-proton collisions collected by the Compact Muon Solenoid (CMS) detector in the LHC. ``Interesting'' events are selected by the first level of the CMS trigger system which uses information from the detector's calorimeters and muon detectors to select events for analysis in a fixed time interval of less than 4 $\mu s$. These events are then further processed by a high-level trigger processor farm, which decreases the event rate from around 100 kHz to less than 1 kHz, before the data is stored. Finally, the particle-flow algorithm reconstructs and identifies all particles from the events selected by the CMS trigger system. With the data properly processed and promptly reconstructed ``online,'' further analysis can be carried out ``offline.''
\end{subsection}
\begin{subsection}{Baseline Selection}\label{baseline-selection}
To start offline analysis, we first apply a baseline selection on data and Monte Carlo samples to filter out particularly irrelevant events. First, we require at one ``good" lepton, qualified as:
\begin{itemize}
    \item $p_{T}(\ell^{\pm}) > 35\textnormal{ GeV}$
    \item $|\eta(\ell^{\pm})| < 2.4$
    \item $ID(\ell^{\pm}) =$ \verb|Medium POG ID|
    \item $I_{mini}(e^{\pm}) < 0.1$
    \item $I_{mini}(\mu^{\pm}) < 0.2$
\end{itemize}

\noindent We also require one good photon, qualified as:
\begin{itemize}
    \item $p_{T}(\gamma) > 30\textnormal{ GeV}$
    \item $|\eta(\gamma)| < 2.5$
    \item $ID(\gamma)$ = \verb|Medium POG ID|
    \item $I_{rel}(\gamma) < 0.06$
    \item $\Delta R(\gamma, \textnormal{any } e^{\pm}) > 0.2$
\end{itemize}
\noindent where if two or more good $\ell$ or $\gamma$ candidates are found, the candidate with the highest $p_{T}$ is selected. Finally, we require two, oppositely charged hadrons. This is complicated by the fact that CMS does not distinguish pions and kaons in reconstruction, but we begin by requiring one good hadron ($h$) candidate pair ($h^{+}, h^{-}$), which are assumed to be the daughters of some unindentified mother meson ($M$), qualified as:
\begin{itemize}
    \item $p_{T}(h^{\pm}) > 35\textnormal{ GeV}$
    \item $\eta(h^{\pm}) < 2.4$
    \item $h^{+}, h^{-}$ from primary vertex
    \item $I_{rel}(M) < 0.06$
    \item $\Delta R(h^{+}, h^{-}) < 0.1$
\end{itemize}
\noindent where $I_{rel}(M) = \frac{max(I(h^{+}), I(h^{-}))}{p_{T}(M)}$. Now, as mentioned previously, the the exact identity of these hadrons is ambiguous. They are saved, by default, as pions, so their four-momenta are all constructed using the pion's mass contribution to the energy component. We add these to form the $\rho$-candidate four-momentum and designate it as the ``pion hypothesis." Then, we manually set the energy-components of the hadron four-momenta using the kaon mass, add them to form the $\phi$-candidate four-momentum, and designate it as the ``kaon hypothesis." If two or more of either hypothesis is found, we select the hypothesis that is closest to the true, respective mass. This leads to the possibility of biasing the data, but we avoid this later by rejecting any events that had more than one meson candidate. Finally, for 2018 data and MC, we exclude any events with $e, \gamma$ in the HEM region, where a portion of the Hadronic Calorimeter was malfunctioning for a particular period of time.
\end{subsection}
\end{section}

\begin{section}{Backgrounds}\label{backgrounds}
We are looking for two final-state particles: one photon and one $\rho$ or $\phi$ meson. However, there are a number of processes besides the Higgs decays we are interested in that could present the same signature. In the following paragraphs, I describe how we might get false photons and false mesons that, when combined, present a signal-like signature.

We may get a real, prompt photon in W events where a photon was radiated by one of the initial-state quarks (Fig. \ref{fig:whiggs-prod}). We might also get a fake photon from misidentified jets in W events. Finally, from Z events, we can get fake photons from misidentified electrons.

Fake mesons may be produced by any two tracks that happen to both be isolated from all particles other than each other and have a combined invariant mass that lands in the $\rho$ or $\phi$-meson Breit-Wigner lineshapes.
\end{section}

\begin{section}{Boosted Decision Tree}
\begin{subsection}{Training}\label{training}
Prior to training, we made the following cuts that prevent over-training the BDT by essentially pre-training it to cut on each respective variable:

\begin{itemize}
    \item $1.0 < m_{K^{+}K^{-}} < 1.04\textnormal{ GeV}$ or $0.5 < m_{\pi^{+}\pi^{-}} < 1.1\textnormal{ GeV}$
    \item $80 < m_{\ell\gamma} < 95\textnormal{ GeV}$
\end{itemize}

\noindent where each was chosen based on the width of their distributions. Then, because BDT's are vulnerable to low statistics, we also added several extraneous, orthogonal datasets that were unnecessary for the greater analysis, but served as useful, accurate background shapes.

We selected the XGBoost python package, a well-known implementation of gradient-boosted decision trees. Twenty features were selected from those saved during the baseline selection step based on their merit as variables that are reasonably uncorrelated to the reconstructed Higgs boson mass. The unweighted distributions for all input features are shown in Fig. \ref{fig:bdt-vars}, but they are also listed below divided into categories for brevity and later reference:
\begin{enumerate}[(i.)]
    \item Missing Transverse Energy ($\slashed{E}_{T}$):
    \begin{itemize}
        \item $p_{T}(\slashed{E}_{T})$
        \item $\varphi(\slashed{E}_{T})$
    \end{itemize}
    \item Basic Kinematics:
    \begin{itemize}
        \item $p_{T}(\gamma)$, $p_{T}(h^{+}h^{-})$
        \item $\eta(\gamma)$, $\eta(h^{+}h^{-})$, $\eta(\ell^{\pm})$
        \item $\varphi(\gamma)$, $\varphi(h^{+}h^{-})$, $\varphi(\ell^{\pm})$
        \item $\Delta R(\gamma, \ell^{\pm})$, $\Delta R(h^{+}h^{-}, \gamma)$, $\Delta R(h^{+}, h^{-})$
    \end{itemize}
    \item Masses:
    \begin{itemize}
        \item $m_{\ell\gamma}$, $m_{h^{+}h^{-}}$
    \end{itemize}
    \item MELA ``Magic" Angles:
    \begin{itemize}
        \item $\Phi$, $\Phi_{1}$
        \item $\cos\theta_{1}$, $\cos\theta_{2}$, $\cos\theta^{*}$
    \end{itemize}
\end{enumerate}

\noindent Now, there are some important clarifications to make here. First, we scaled all $p_{T}$ quantities that are correlated to the Higgs and input into the BDT by the Higgs mass. Second, $\varphi$ refers to the azimuthal angle (see Fig. \ref{fig:cms-coords}) which is not to be confused with the $\phi$-meson. Finally, we calculated the quantities listed under (iv.) using MELA\cite{cite-mela} (all angles used are illustrated in Fig. \ref{fig:magic-angles}).

\begin{figure}[htb]
\begin{center}
\includegraphics[width=.95\linewidth]{Dissertation/fig/magic-angles.pdf}
\end{center}
\caption{Diagram due to Anderson et. al. \cite{magic-angles-cite} of Higgs-frame angles used for BDT training. The center diagram is most relevant under the following replacements: $Z, Z^*$ to $W, W^*$, $b, \bar{b}$ to $\rho/\phi, \gamma$; $\ell^+,\ell^-$ to $e^-/\mu^-, \nu_e/ \nu_\mu$.}
\label{fig:magic-angles}
\end{figure}

With these features properly defined, we ran the BDT for 200 training rounds with the following model hyperparameters selected to maximize BDT efficiency without overtraining (See Fig. \ref{fig:bdt-knobs} for hyperameter definitions):
\begin{itemize}
    \item \verb|objective| = 'binary:logistic'
    \item \verb|eta| = 0.1
    \item \verb|max_depth| = 3
    \item \verb|verbosity| = 1
    \item \verb|nthread| = 12
    \item \verb|eval_metric| = "auc"
    \item \verb|subsample| = 0.6
    \item \verb|alpha| = 8.0
    \item \verb|gamma| = 2.0
    \item \verb|lambda| = 1.0
    \item \verb|min_child_weight| = 1.0
    \item \verb|colsample_bytree| = 1.0
\end{itemize}

\end{subsection}
\begin{subsection}{Performance and Validation}\label{perf-and-val}
We determined satisfactory performance by evaluating the BDT's ROC curves for testing and training (Fig. \ref{fig:bdt-performance}) as well as the sanity of the BDT's feature rankings (Fig. \ref{fig:bdt-vars}). We also checked for background sculpting by two methods. First, we looked at the plot of the BDT scores versus the reconstructed Higgs mass (Fig. \ref{fig:bdt-bkgsculpt1}). A correlation between high BDT scores and the known Higgs mass (125 GeV) would indicate that the BDT was simply learning and cutting on the reconstructed Higgs mass and thus sculpting the background. Second, we directly evaluated the background and signal distributions before and after making a tight cut ($D > 0.9$) on the BDT discriminant, where an artificial peak of the background inside of the signal region would directly show that the BDT was sculpting the background. We required that the BDT pass these two tests before proceeding to use it in the rest of the analysis.

\begin{figure}[htb]
\begin{center}
\input{Dissertation/fig/bdt-vars.tex}
\end{center}
\caption{Top ten variables as ranked by the BDT by gain. All input variables are reconstruction-level Monte Carlo data, and all $p_{T}$ variables are scaled by $m_{h^{+}h^{-}\gamma}.$}
\label{fig:bdt-vars}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=.95\linewidth]{Dissertation/fig/bdt-performance.png}
\end{center}
\caption{Left: ROC curve showing BDT testing (blue) and training (orange) performance. Right: Background (blue) and signal (red) distributions versus BDT score for testing (outline) and training (filled).}
\label{fig:bdt-performance}
\end{figure}

\end{subsection}
\begin{subsection}{Comparison to Cut-Based Approach}\label{comp-to-cuts}
Based on the following basic study, we found that a Boosted Decision Tree (BDT) trained on Monte Carlo simulations of signal and background performed 10\% to 15\% better than traditional, cut-based methods. We determined an optimal BDT working point by sampling the BDT's ROC curve (generated using testing data and predictions) at each defined threshold value, then calculating an expected significance ($\sigma$) defined as:
\begin{equation}\label{expsig-eq}
    \sigma = \sqrt{2(s+b)\ln(1+s/b)-2s}
\end{equation}
\noindent where $s$ and $b$ are the number of signal and background events for a given threshold value. The optimal BDT working point is then given by the maximum $\sigma$-value. Next, we defined a general cut-based approach by the following cuts:
\begin{itemize}
    \item $1.0 < m_{h^{+}h^{-}} < 1.04\textnormal{ GeV}$
    \item $\Delta R(h^{+}, h^{-}) < 0.015$
    \item $p_{T}(h^{+}h^{-}) > 25\textnormal{ GeV}$
    \item $p_{T}(\gamma) > 40\textnormal{ GeV}$
    \item $I_{rel}(h^{+}h^{-}) < 0.01$
\end{itemize}
\noindent Finally, we calculated the false-positive and true-positive rates of the cut-based methods and plotted them against the BDT's ROC curve. The results of this study are plotted in Fig. \ref{fig:bdt-vs-cuts}. Note that the terms BDT ``score," ``threshold," and ``discriminant" have been and will be used interchangeably throughout the remainder of this paper with the understanding that they are the same quantity $D \in [0,1]$.

\end{subsection}
\begin{subsection}{Optimization for Data}\label{optim-for-data}
With the BDT validated, we proceeded to calculated a proper BDT working point for cutting on data. We noticed that the Monte Carlo samples were not properly modeling the data in the signal region of the BDT discriminant (Fig. \ref{fig:bdt-thresh-dist}). Thus, we concluded that we could not sample the BDT testing ROC curve for an effective working point. Instead, we fed the BDT ROC curve data as background in addition to signal Monte Carlo. We then sampled this distribution, calculated expected significance again using Eq. \ref{expsig-eq}, and took the best BDT working point to be at the threshold with maximum expected significance (Fig. \ref{fig:bdt-data-expsig}).

\begin{figure}[htb]
\begin{center}
\subfloat      {
\includegraphics[width=.45\linewidth]{Dissertation/fig/bdt-threshBySample.png}
}\quad
\subfloat      {
\includegraphics[width=.45\linewidth]{Dissertation/fig/bdt-threshBySample_rho.png}
}
\end{center}
\caption{Stacked BDT discriminant distribution plot by sample. (Left) $H\rightarrow\phi\gamma$ analysis. (Right) $H\rightarrow\rho\gamma$ analysis.}
\label{fig:bdt-thresh-dist}
\end{figure}

\end{subsection}
\end{section}

\begin{section}{Results}
After cutting on the best BDT working point described in the previous section, we fit the background to an exponential function and the signal to a single Crystal Ball function. Then, we used the HiggsCombine tool's asymptotic limits calculations\cite{cite-hcomb} to generate the plots in Fig. \ref{fig:final-fits} as well as the exclusion limits listed in Fig. \ref{fig:exclude-lims} (at the 95\% confidence level). We see that our result is within one order of magnitude of that produced by ATLAS in a study\cite{cite-rpg-brs} of these same decays, but through gluon-gluon fusion using a dedicated trigger. However, a weaker result was expected. Although we had approximately four times the amount of data from the full Run II 137 fb\textsuperscript{-1} dataset, the much smaller cross-section of the associated $WH$ production compared to gluon-gluon fusion already results in a reduction in cross-section by a factor of about 32. On top of that, the branching ratio for $W\rightarrow e^{\pm}/\mu^{\pm}\nu_{e,\mu}$ is approximately 22\%, which reduces the effective $WH$ cross-section and results in an overall reduction in cross-section by a factor of 146.

\begin{figure}[htb]
\centering
\begin{tabular}{ccc}
\toprule
Branching Ratio & Expected & Observed \\
\midrule
$\mathcal{B}(H\rightarrow\phi\gamma)$ & $(5.2^{+3.0}_{-1.8})\times10^{-3}$ & $4.1\times10^{-3}$ \\
$\mathcal{B}(H\rightarrow\rho\gamma)$ & $(4.1^{+2.3}_{-1.3})\times10^{-3}$ & $7.0\times10^{-3}$ \\
\bottomrule
\end{tabular}
\caption{Exclusion limits for the two analyses covered by this paper.}
\label{fig:exclude-lims}
\end{figure}

\begin{figure}[htb]
\begin{center}
\subfloat[]      {
\includegraphics[width=.45\linewidth]{Dissertation/fig/finalfit_rho.pdf}
}\quad
\subfloat[]      {
\includegraphics[width=.45\linewidth]{Dissertation/fig/finalfit_phi.pdf}
}\\
\subfloat[]      {
\includegraphics[width=.45\linewidth]{Dissertation/fig/finalfit_rho_bgOnlyFit.pdf}
}\quad
\subfloat[]      {
\includegraphics[width=.45\linewidth]{Dissertation/fig/finalfit_phi_bgOnlyFit.pdf}
}
\end{center}
\caption{Final fits for the $H\rightarrow\rho\gamma$ (left) and $H\rightarrow\phi\gamma$ (right) analyses.}
\label{fig:final-fits}
\end{figure}

\end{section}